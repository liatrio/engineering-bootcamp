---
docs/3-AI-Engineering/3.3.1-agentic-best-practices.md:
  category: AI Engineering
  estReadingMinutes: 40
---

# AI Development for Software Engineers

AI-enhanced software development is quickly becoming the norm, but effectively leveraging these tools requires more than just casual prompting. While it's tempting to ask an AI to "create me an Instagram clone from scratch," the result will likely be an unmanageable mess that fails to meet production standards. Thoughtful, structured use of these tools leads to more maintainable and reliable outcomes. This section provides engineers with advanced techniques for integrating AI into their development workflow while maintaining engineering rigor and oversight, offering a practical framework for getting the most value from these powerful tools.

![No Easy Button](img3/no-easy-button.png ":size=600 :alt=No Easy Button")

## Thoughtful AI Development

Instead of starting with the "Make me an Instagram clone" prompt we still need to start with thoughtful planning. This section introduces **Spec-Driven Development (SDD)**, a structured four-stage workflow that transforms "vibe-based" AI usage into disciplined, engineering-focused development practices.

Much of this approach is informed by Liatrio's [Spec-Driven Workflow](https://github.com/liatrio-labs/spec-driven-workflow) methodology and HumanLayer's "No Vibes Allowed" principles for structured AI-assisted development. As LLMs and the tools around them continue to improve, this workflow provides a stable foundation that emphasizes engineering rigor, incremental validation, and proof-driven implementation.

### No Vibes Allowed: Structured AI Development

The following video introduces the "No Vibes Allowed" approach—moving from exploratory, unstructured AI usage to systematic, verifiable development practices:

[video](https://www.youtube.com/watch?v=IS_y40zY-hc)

For an alternative recording with additional perspectives, see [this version](https://www.youtube.com/watch?v=rmvDxxNubIg) of the same talk.

The four-stage SDD workflow below applies these principles to real-world development. These practices work for both greenfield and brownfield projects, though brownfield implementations focus the planning phase on the specific task at hand rather than full system design.

### 1. Generate Specification (SDD Stage 1)

The first stage of Spec-Driven Development transforms a high-level idea into a comprehensive, developer-ready specification. This specification becomes the source of truth for your implementation, defining goals, requirements, constraints, and success criteria before any code is written.

**Purpose:**
- Convert informal ideas into structured, actionable specifications
- Establish clear requirements and constraints upfront
- Create a reference document for validation and verification
- Enable parallelization: multiple developers can implement from the same spec
- Provide context for AI assistants throughout development

**The Clarifying Questions Process:**

Use a conversational LLM to iteratively refine your idea through a question-based dialogue. The AI should ask one question at a time, building on your previous answers to extract every relevant detail.

**Example Spec Generation Prompt** (adapted for DevOps Bootcamp):

```text
I need to create a specification for a DevOps automation project. Ask me one question at a time to develop a thorough, step-by-step spec.

Each question should build on my previous answers. Our end goal is a detailed specification covering:
- Project goals and success metrics
- Functional and non-functional requirements
- Architecture and technology choices
- Security and compliance considerations
- Testing strategy
- Deployment approach

Only one question at a time. When giving options, format them in a numbered list.

Here's the idea:
[YOUR_IDEA]
```

**After Completing the Brainstorming:**

```text
Now compile our findings into a comprehensive, developer-ready specification. Include:
- Executive summary
- Goals and non-goals
- User stories or use cases
- Demoable units of work with proof artifacts
- Technical considerations
- Security and compliance requirements
- Success metrics
- Open questions (if any remain)

Format this as a structured markdown document that a developer can immediately use for implementation planning.
```

**Save and Commit the Specification:**

Save this as `spec.md` or following your project's naming convention (e.g., `[NN]-spec-[feature-name].md`).

Practice small batch delivery by committing this spec with detailed metadata:

**Example Commit Message:**

```text
docs: add specification for [feature-name]

- Generated: 2025-01-09
- Model: Claude Sonnet 4.5 / GPT-4
- Temperature: 0.7
- Prompt: Iterative clarifying questions workflow
- Notes: Focused on DevOps automation requirements, emphasized security considerations
```

**Resources:**
- [Liatrio Spec-Driven Workflow](https://github.com/liatrio-labs/spec-driven-workflow) - Complete SDD methodology with templates and examples

### 2. Task Breakdown (SDD Stage 2)

The second stage transforms your specification into an executable implementation plan. Break the spec into parent tasks representing demoable units of work, each with clear proof artifacts that demonstrate completion.

**Purpose:**
- Transform specifications into actionable, incremental tasks
- Create parent tasks that represent meaningful milestones
- Define proof artifacts that validate completion
- Identify relevant files and dependencies
- Generate sub-tasks that build toward parent task completion

**Breaking Specs into Demoable Units:**

Each parent task should represent work that can be demonstrated, tested, and validated independently. Parent tasks should:
- Deliver working functionality (not partial implementations)
- Have clear, verifiable proof artifacts
- Take 2-8 hours of focused implementation time
- Build logically toward the overall spec goals

**Creating Parent Tasks with Proof Artifacts:**

Proof artifacts provide evidence that a task is complete and working as specified. Common proof artifacts include:
- CLI output showing successful execution
- Test results demonstrating functionality
- Screenshots of UI features
- Configuration files showing correct setup
- Performance metrics or logs
- API response examples

**Example Task Breakdown Prompt:**

```text
Given this specification, break it into parent tasks that represent demoable units of work. For each parent task:

1. Provide a clear purpose statement
2. List functional requirements it satisfies
3. Define proof artifacts that demonstrate completion
4. Identify relevant files that will be modified or created
5. Break down into 3-8 sub-tasks that build toward completion

Each parent task should deliver working, testable functionality.

<YOUR_SPEC>
```

**Example Task Structure:**

```text
## Task 1.0: Implement Authentication Middleware

**Purpose**: Add JWT-based authentication to protect API endpoints

**Proof Artifacts:**
- CLI output: Successful authentication with valid JWT
- CLI output: 401 Unauthorized with invalid/missing JWT
- Test results: All auth middleware tests passing
- Configuration: Environment variables documented

**Relevant Files:**
- src/middleware/auth.ts (new)
- src/routes/api.ts (modify)
- tests/middleware/auth.test.ts (new)

**Sub-tasks:**
1. Create JWT validation utility function
2. Implement authentication middleware
3. Add middleware to protected routes
4. Write unit tests for middleware
5. Write integration tests for protected endpoints
6. Document authentication setup in README
```

**Save and Commit:**

Save this as a tasks file (e.g., `[NN]-tasks-[feature-name].md`). Commit with appropriate metadata:

```text
docs: add task breakdown for [feature-name]

- Generated: 2025-01-09
- Model: Claude Sonnet 4.5
- Based on: [NN]-spec-[feature-name].md
- Notes: Organized into 6 parent tasks with proof artifacts
```

**Alternative Tools:**

For automated task generation, consider [TaskMaster AI](https://www.task-master.dev/) which provides additional features for task management and includes an MCP server for integration with AI assistants.

### 3. Execute with Management (SDD Stage 3)

The third stage executes your task list incrementally while maintaining engineering rigor, context hygiene, and continuous validation. This stage emphasizes single-threaded execution, proof artifact collection, and proactive context management.

**Purpose:**
- Implement tasks systematically, one sub-task at a time
- Verify functionality at each checkpoint before proceeding
- Maintain proof artifacts as evidence of completion
- Manage context utilization to prevent degradation
- Commit frequently with clear, traceable messages

**Single-Threaded Execution:**

Work on exactly one parent task at a time, completing all its sub-tasks before moving to the next. This approach:
- Maintains clear focus and reduces context switching
- Enables meaningful commits at parent task boundaries
- Facilitates proof artifact collection
- Allows for mid-implementation course corrections

**Verification Checkpoints:**

After completing each sub-task:
1. **Test the functionality**: Run relevant tests or manual verification
2. **Check code quality**: Run linters, formatters, type checkers
3. **Review implementation**: Ensure it meets requirements and follows patterns
4. **Update task file**: Mark sub-task as complete immediately

**Context Management During Implementation:**

As you work through tasks, actively manage context utilization to maintain AI effectiveness (see [AI Best Practices](3.1.4-ai-best-practices.md#context-rot-and-performance-degradation) for detailed coverage):

- **Monitor context**: Use tools like `/context` in Claude Code or check context indicators in your AI assistant
- **Watch for 40%+ utilization**: Performance degradation begins around 40% context utilization
- **Trigger compaction at 60%+**: When context exceeds 60%, apply intentional compaction before proceeding
- **Phase transitions**: Natural compaction points occur between parent tasks (research → planning → implementation → testing)

**Compaction Workflow:**

```text
1. Recognize: Context exceeding 60% or noticing degradation symptoms
2. Summarize: "Summarize our progress: completed tasks, current state, next steps"
3. Start fresh: New conversation with summary as context
4. Load selectively: Add only files/context needed for current task
5. Continue: Resume implementation with restored AI effectiveness
```

**Committing After Each Parent Task:**

Create a git commit after completing each parent task:

```text
feat: implement authentication middleware

- Add JWT validation utility function
- Implement auth middleware with error handling
- Integrate middleware into protected routes
- Add comprehensive unit and integration tests
- Document authentication setup and environment variables

Proof artifacts in docs/specs/[NN]-spec-[feature]/[NN]-proofs/[NN]-task-01-proofs.md

Related to T1.0 in Spec [NN]
```

**Maintaining Proof Artifacts:**

Create proof artifacts as you complete parent tasks. Collect:
- CLI output demonstrating functionality
- Test results showing all tests passing
- Screenshots for UI features
- Configuration examples
- Performance metrics or logs

Save as `[NN]-task-[TT]-proofs.md` in the spec's proofs directory.

**Leveraging IDE Agentic Capabilities:**

Modern IDEs provide capabilities that enhance implementation:
- **Instruction/rule files** (CLAUDE.md, .cursorrules): Project-specific context and conventions
- **MCP servers**: Integration with external tools and services
- **Web-based docs**: On-demand access to framework documentation
- **Context-aware prompting**: Relevant file and symbol information

These capabilities vary by tool (VSCode with Copilot, Cursor, Windsurf, Claude Code) but share the principle of providing structured context to AI assistants.

**Adapting for Existing Codebases:**

For brownfield projects:
1. **Understand existing patterns**: Read relevant code before implementing
2. **Identify integration points**: Locate where new code connects to existing systems
3. **Test incrementally**: Verify changes don't break existing functionality
4. **Match conventions**: Follow established naming, structure, and testing patterns

### 4. Validate Implementation (SDD Stage 4)

The final stage validates that your implementation fully satisfies the original specification. This systematic review ensures nothing was missed, all proof artifacts demonstrate required functionality, and the implementation is ready for deployment or handoff.

**Purpose:**
- Verify all spec requirements are satisfied
- Validate proof artifacts demonstrate functionality
- Ensure code quality and testing standards are met
- Confirm documentation is complete and accurate
- Identify any gaps or rework needed

**Validating Against the Specification:**

Compare your implementation against the original spec systematically:

1. **Review each requirement**: For every requirement in the spec, verify corresponding implementation exists
2. **Check success criteria**: Ensure all success metrics defined in the spec are met
3. **Validate constraints**: Confirm technical, security, and operational constraints are satisfied
4. **Test edge cases**: Verify behavior for boundary conditions and error scenarios

**Reviewing Proof Artifacts:**

Examine proof artifacts for completeness and accuracy:

- **Functionality proof**: Does the artifact demonstrate the feature works as specified?
- **Quality proof**: Are tests passing? Does linting pass?
- **Integration proof**: Does the feature work with existing systems?
- **Performance proof**: Do metrics meet specified thresholds?

**Coverage Matrix:**

Create a simple coverage matrix showing spec requirements mapped to implementations and proof artifacts:

```text
| Requirement | Implementation | Proof Artifact | Status |
|-------------|----------------|----------------|--------|
| JWT authentication | src/middleware/auth.ts | Task 1 proofs, lines 12-45 | ✓ Complete |
| 401 on invalid token | src/middleware/auth.ts:67-89 | Task 1 proofs, lines 78-92 | ✓ Complete |
| Environment config | .env.example, README.md | Task 1 proofs, lines 120-135 | ✓ Complete |
```

**Final Validation Checklist:**

- [ ] All spec requirements implemented
- [ ] All parent tasks completed and committed
- [ ] All proof artifacts created and reviewed
- [ ] Test suite passing (unit, integration, e2e as applicable)
- [ ] Code quality gates passing (linting, type checking, formatting)
- [ ] Documentation updated (README, API docs, inline comments where needed)
- [ ] Security considerations addressed (no credentials committed, input validation, etc.)
- [ ] Performance requirements met (if specified)

**Addressing Gaps:**

If validation reveals gaps:
1. **Document the gap**: What requirement is not fully satisfied?
2. **Assess severity**: Is this blocking deployment/handoff?
3. **Create remediation task**: Add to task list with appropriate priority
4. **Implement fix**: Follow same SDD workflow (update spec → create task → implement → validate)
5. **Re-validate**: Ensure gap is closed before considering work complete

Once validation passes, your implementation is ready for code review, deployment, or handoff to stakeholders.

## Other Practical AI Techniques

These techniques leverage AI's strengths while mitigating its weaknesses through careful engineering practices. They are suitable for both greenfield and brownfield projects.

### 1. The "Second Opinion" Technique

![Cage Match](img3/cage-match.jpg ":size=600 :alt=Cage Match")

Pit multiple LLMs against each other. Task another LLM to review your existing technical work searching for simpler or more idiomatic solutions.

**How to use**:
- Copy your artifact (code, architecture plan, etc.) into an AI chat
- Request feedback focused on simplicity, clarity, or best practices
- Apply insights that genuinely improve your implementation

**Example Prompts**:

```text
What do you think of this implementation? Can you suggest any simpler or more idiomatic ways to achieve the same result?
```

```text
Here is a draft architecture plan. Are there any obvious complexities I've introduced, or simpler patterns I might have overlooked?
```

### 2. The "Throwaway Debugging Scripts" Technique

Take advantage of LLMs ability to generate short, functional scripts to automate debugging steps without requiring codebase integration.

**How to use**:
- Identify specific debugging steps you would perform manually
- Ask the LLM to write a script (10-30 lines) to perform these steps
- Use the script as a disposable tool to diagnose issues

**Example Scenarios**:

```text
Generate a Python script that executes sub-parts of this Elasticsearch query sequentially
to help identify which part is causing unexpected results. The script should:
1. Connect to our Elasticsearch cluster at <ES_ENDPOINT>
2. Execute each major clause of the query independently
3. Print the number of results and a sample document for each test
```

```text
Create a Python script using the `requests` library to test different API endpoints
with various query parameters. The script should:
1. Make GET requests to <BASE_URL>/api/endpoint with different combinations of parameters
2. Log the response status code and response time for each request
3. Save the full response when the status code is not 200
4. Include proper error handling and timeouts
```

You should not spend too many cycles on this and they should be short and crude. The idea is to speed up your local testing not build a new standard way of debugging.

### 3. Plugging Technical Gaps

Quickly get up to speed in unfamiliar domains by using LLMs to generate initial code or configuration snippets.

![I Know Kung Fu](img3/i-know-kung-fu.gif ":class=img-center :size=400 :alt=I Know Kung Fu")

**Example Scenario**:

```text
Generate an nginx configuration block to reverse proxy requests from /api/* to http://backend-service:8080, ensuring that the Host header is preserved.
```

```text
Create a production-ready multi-stage Dockerfile for a Python FastAPI application that includes:
1. A build stage with all build dependencies
2. A final stage using a slim Python image
3. Proper layer caching for faster builds
4. Non-root user for security
5. Health checks
6. Proper signal handling for graceful shutdowns. The app uses Poetry for dependency management and needs to connect to both PostgreSQL and Redis services.
```

**Important**: Always have code generated in unfamiliar areas reviewed by a domain expert.

## Documenting Your Prompts

Given the variability of LLM outputs across models, settings, and versions, documenting your prompts is crucial for effective prompt engineering.

### Why Document?

- Provides a record of what prompts were used and what results were obtained
- Essential for revisiting work and troubleshooting unexpected outputs
- Helps test prompts on new model versions
- Facilitates knowledge sharing across teams

### What to Document

- Prompt text
- Model used (and version)
- Configuration settings (temperature, Top-K/Top-P, etc.)
- Date and context
- Expected vs. actual output
- Evaluation notes

### Best Practices

- Save prompts in version control for operationalized systems
- Use a structured format (e.g., a table in a markdown file)
- Include examples of good and bad outputs
- Document prompt engineering iterations and their results

## Maintaining the "Dumb Tool" Perspective

Always remember that LLMs are statistical models guessing the next token, not intelligent beings with understanding.

### How to Apply

- **Ask Clearly Bounded Questions**: Frame prompts with constrained expected outputs that are easy to verify
- **Remain the Decision-Maker**: Generate possibilities with AI, but retain decision authority
- **Leverage Objectivity**: Get unfiltered feedback based on patterns in training data, not personal opinion

### Practical Example

Instead of:
```text
"Write me the code for feature X"
```

Follow this workflow:
1. Brainstorm the specification first
2. Create a detailed implementation plan
3. Use AI for small, well-defined code generation tasks
4. Review and test each piece before integration

For feedback, be specific:
```text
Provide a thorough code review including line numbers and contextual information. This will be passed to a teammate, so be precise and don't hallucinate.
```

## Knowledge Check

<div class="quizdown">
  <div id="chapter-3/3.3/agentic-best-practices-quiz.js"></div>
</div>

## Deliverables

- Describe the four stages of the SDD workflow and what each stage produces.
- How does the SDD approach differ from "vibe-based" AI development?
- What are proof artifacts, and why are they important in the SDD workflow?
- When would you trigger intentional compaction during the Execute with Management stage?
- How would you adapt the SDD workflow for a brownfield (existing codebase) project versus a greenfield (new) project?
- Which of the "Other Practical AI Techniques" (Second Opinion, Throwaway Debugging Scripts, Plugging Technical Gaps) have you used before, and in what contexts?
